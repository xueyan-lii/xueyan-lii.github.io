---
title: "Adaptive Temperature Scaling: Does It Actually Work?"
collection: publications
category: ongoing
permalink: /publication/2025-06-23
excerpt: "We train a transformer or MLP layer to output step-wise temperature or correctness signals. We deep dive into an existing method ATS and find multiple implementation errors. We fix these errors and improve them in terms of generalizability and versatility. Work in progress."
date: 2025-06-23
link: https://github.com/xueyan-lii/adaptive-head
---

We find the following implementation flaws in the [code](https://github.com/johnathan-xie/adaptive-temperature-scaling) for the paper [Calibrating Language Models with Adaptive Temperature Scaling](https://arxiv.org/abs/2409.19817).
- Generation does not use their custom forward function, meaning no temperature is used with do_sample. They patch it post generation to get the adapted logits again​. This is an okay but not exact approximation for generation tasks with short answers.
- Calibration head has kv-cache incorrectly used and receives only inputs tokens of the last step, from the second generation step onwards. It receives the full input sequence for the first step​ **only**. This means that the calibration head can only do **self-attention** and do not see the full context, which betrays the purpose of using a transformer architecture altogether.
- An additional T=0.6 scaling is inserted into the sample function at the last step. So the adaptive temperature found through their transformer head is multiplied by 0.6 again which makes all final temperatures used lower? Why? This is not mentioned in their methods section.
- Unexplained loss function setup. Smooth loss is weighed by number of correct tokens, around 70% which results in higher weighting​. Since question tokens are not masked, the number of incorrect samples overcounted by the number of question tokens. Smooth loss weight decreases​. These two setups results in smoothing loss weight being pulled in opposite directions.
- Cannot replicate performance.

There are additional flaws in the experiment setup
- All evaluated benchmarks have short answers. MMLU is MCQ. TriviaQA has short answers​. Unclear if this method would scale to longer answers.
- Paper comparison to methods in literature but do not mention hyperparameters, even though comparisons are non-trivial​.

We fix implementation issues in ATS by bootstraping on torchtune and improves in terms of efficiency and versatility
- Allow for batching, padding all sequences to the same length, kv-caching, masking question tokens for training. These standard functionalities were not possible in the original codebase.
- Use custome forward function for step-wise temperature based sampling. 
- Kv cache is enabled for transformer head, meaning it will attend to all previous tokens, rather than just the last one.
- Loss function has smooth loss weighted by the number of incorrect tokens. Question tokens are masked. These are non-conflicting and intuitive implementations as described by their original paper.
- We conduct fine-tuning and evaluation pipeline on long sequence generation tasks like GSM8K.