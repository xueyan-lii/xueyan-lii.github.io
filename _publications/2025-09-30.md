---
title: "Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs"
collection: publications
category: conferences
permalink: /publication/2025-09-30
excerpt: 'We question common assumptions in literature about how high model uncertainty warrants more exploration. We find that existing work confound epistemic and aleatoric uncertainty and propose a counter-intuitive sampling strategy. Under review for ICLR 2026.'
date: 2025-09-30
paperurl: 'https://arxiv.org/abs/2510.05987'
---

Large Language Models (LLMs) are increasingly applied to complex tasks that require extended reasoning. In such settings, models often benefit from diverse chains-of-thought to arrive at multiple candidate solutions. This requires two competing objectives: to inject enough stochasticity to explore multiple reasoning chains, and to ensure sufficient accuracy and quality in each path. Existing works pursue the first objective by increasing exploration at highly uncertain steps with higher temperature or larger candidate token sets, while others improve reliability by rejecting samples with low confidence post-generation, implying that low confidence correlates with low answer quality. These two lines of thought are in conflict, as they conflate different sources of uncertainty. To resolve this, we argue that the decoding rule should be calibrated by correctness, not confidence alone. We should sample from tokens with higher estimated correctness, and reduce sampling where expected correctness is low. We propose simple strategies that achieve this goal: Greedy-Threshold makes sampling greedy at very low confidence steps. Calibrated-TopK and Calibrated-epsilon set truncation threshold based on estimated rank-wise correctness. Together, our findings challenge prevailing heuristics about decoding under uncertainty, showing consistent gains across math and general reasoning benchmarks.