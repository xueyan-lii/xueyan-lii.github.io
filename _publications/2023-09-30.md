---
title: "Retrieve, Summarize, Decide: A Two-Stage Recipe for Knowledge Based Visual Question Answering"
collection: publications
category: thesis
permalink: /publication/2023-09-30
excerpt: "MPhil thesis project at University of Cambridge supervised by Prof. Bill Byrne. Obtained score 87%. [Full paper](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ON5WOc4AAAAJ&authuser=1&citation_for_view=ON5WOc4AAAAJ:d1gkVwhDpl0C) • [Short paper](/files/vqa_short.pdf) We find limitations in existing retrieval augmented visual question answering methods, and propose a novel pipeline that make use of common methods in literature. We added various visual encoders (CLIP, BLIP2, InstructBLIP) to retrieval-augmented large language models. Performance on the OKVQA dataset outperforms all other retrieval-based models. Frozen LLMs (e.g. GPT-3.5) are used to choose a final answer from document-based answer candidates with in-context learning. Performance outperforms all other in-context methods."
date: 2023-09-30
---

Knowledge-based visual question answering (KBVQA) requires reasoning over an image together with knowledge that cannot be inferred from the image alone. Prior work typically relies on one of three tools: large vision–language models (VLMs), external document retrieval, or in-context prompting of frozen LLMs. Each of these methods has limitations when used in isolation. We present a two-stage framework that unifies all three approaches: **Stage 1:** We augment retrieval-augmented VQA with question-aware visual encoding (InstructBLIP) and train the retriever and generator jointly to produce *document-wise* answer candidates. These are concise, question-focused summaries grounded in retrieved passages. **Stage 2:** We prompt a separate frozen LLM with few-shot examples that combine question-aware captions (PromptCap) and the Stage 1 candidates. This allows the model to either select a grounded candidate or fall back to its implicit knowledge when appropriate. On OKVQA, our Stage 1 model reaches a 62.83% VQA score, surpassing prior retrieval-based systems. Our Stage 2 method attains 61.69% with only 5-shot prompting and no ensembling, outperforming comparable in-context approaches.